「マルチヘッドアテンション（Multi-Head Attention）」は、**Transformerモデルの中核**をなす重要な仕組みの一つです。

---

##  1. アテンションとは？

まず前提として、「アテンション（注意）」とは、ある入力（例：文章の単語）に対して、**他の入力のどこに注目すべきかを計算する仕組み**です。

例えば、
>「彼は**リンゴ**を食べた。」

この文で「食べた」が指している対象は「リンゴ」ですよね。アテンションは、こうした「どこに注目すべきか」を学習します。

---

##  2. マルチヘッドアテンションとは？

「マルチヘッド（Multi-Head）」とは、**複数のアテンションを並列で行う**ことを意味します。

###　 なぜ複数のヘッドを使うの？

- 一つのアテンションヘッドだけだと、「一つの視点」でしか注目できない。
- 複数のヘッドを使えば、「異なる視点」で複数の場所に注目可能！
- 結果的に、**より豊かな文脈理解**ができるようになります。

---

##  3. マルチヘッドアテンションの流れ

### ステップ 1：入力ベクトルから Q, K, V を作る  
それぞれの単語ベクトルから、次の3つを作ります：

- **Q（Query）**：注目する側  
- **K（Key）**：注目される側  
- **V（Value）**：情報の中身

### ステップ 2：アテンションを計算  
通常のアテンションは以下の式で計算されます：

\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
\]

### ステップ 3：複数ヘッドで並列処理  
上のアテンションを複数（例えば8個）同時に行います。各ヘッドは別の重み行列を使って Q, K, V を作ります。

### ステップ 4：結合して線形変換  
各ヘッドの出力を結合（concat）して、全体で1つの出力にまとめます。

---

##  4. 図で見る構造（イメージ）

```
[入力] ──▶ Q,K,V ──▶ ┌── ヘッド1 ┐
                     │  ヘッド2  │
                     │   ...     │ → 結合 → 線形変換 → 出力
                     └── ヘッドN ┘
```

---

##  5. 実際に使われる場所

- **Transformer**（BERT, GPT, T5など）
- 自然言語処理（機械翻訳、文書要約、質問応答）
- 画像処理（Vision Transformer）
- 音声認識 など

---

##  まとめ

| 用語                | 意味                           |
|---------------------|--------------------------------|
| アテンション        | 入力間の関連性を計算する仕組み |
| マルチヘッド        | 複数のアテンションを並列実行   |
| 利点                | 多様な視点で文脈を理解できる   |


参考文献：
- Vaswani et al., “Attention is All You Need”, 2017  
- [The Illustrated Transformer（英語）](https://jalammar.github.io/illustrated-transformer/)
